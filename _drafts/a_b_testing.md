A short introduction to one way to analyze A|B tests

In trying to find resources for my colleagues about A|B testing, I came across a really wide variety of approaches for analyzing/setting up A|B tests. In this post, I'll try to explain one of the simplest ones - the Z test with a wald interval. This is the approach used in this post from Amazon about their A|B testing tool [The Math Behind A|B Testing](https://developer.amazon.com/public/apis/manage/ab-testing/doc/math-behind-ab-testing). I should also start by saying that if the content of this post seems a bit basic to you, you're probably not the target audience. Data scientists and statisticians - this post is not for you - it is for your colleagues, many of whom have had some statistics and would benefit from knowing how A|B testing relates to what they learned in school. This post is also not meant to be a full explanation of the intuition behind A|B testing but instead is meant to provide a supplement to more intensive study. I intend to provide a few signposts and key words so that any reader can better navigate all the other great resources out there.

##Let's start with an example.
Say you run a news website which can be browsed without registration, but registering allows readers to get content recommendations and premium content and it also allows you to gather more information that you can use to tailor content to users. Thus, you want to encourage more users to register, so you decide to try adding a short block of text under the registration prompt explaining to users what advantages they will get if they register. Let's call the normal registration prompt Version A and the prompt with the additional information Version B.

##What are we trying to find out with our A|B test.
In our case, what we are interested in knowing which registration prompt results in more users registering, i.e a better conversion rate. It seems like it would be easy enough to just roll out the changed registration prompt to a portion of one's users and compare the conversion rates for each alternative and choose the one with the better conversion rate. And we wouldn't have to use any messy statistics!

You probably know that there are more than a few issues with this, but let's take a closer look at a few so that we can figure out what exactly we gain from proper analysis of an A|B test. 

###Sample Size
This is probably the easier component to reason about. Intuitively, we know that if we see a 10% conversion rate for Version B (registration prompt w/text) but only 10 people total have seen it so far (i.e. 1 person signed up), we can be less sure that that's the real conversion rate than if 100,000 people have seen it and 10% of those converted. It's easy enough to see the difference when we compare two extremes, but it's quite a bit harder when our numbers are somewhere in the middle. We use the concept of "confidence intervals" around a conversion rate to capture this difference in certainty. In the Amazon method, they use standard error to quantify how different we think the true conversion rate could be from what we observe. From the formula for standard error of a conversion rate, p:

	SE = sqrt(p(1-p)/sampleSize)

we can clearly see that a larger sample size (or number of page views, in our case) would result in a smaller standard error and a smaller interval on which we think the true conversion rate lies. In our example above, a 10% conversion rate based on 10 views would have a SE ~= 9.5% vs. a 10% conversion rate based on 100,000 views would have SE ~= 0.095%. It's also worth noting that from this standard error formula we can see that, given the same number of views, we can have more certainty about a conversion rate if it is closer to the extremes - 0% or 100%. This is equivalent to saying that the more distant the success and failure rates are from each other, the smaller the error. The largest error will thus be when the conversion rate is exactly 50%. 

###Randomness
In the previous section, we introduced the idea of the **true** conversion rate, which we distinguish from the conversion rate which we observe in our experiment.In general, when we run an A|B test, we want to be able to say something about this **true** conversion rate. More specifically, I run an A|B test not to find out whether Variation A performs better than Variation B for the subset of people who see the test but for all of my customers. In statistical terms, I want to know about the **population** not just a smaller **sample**. I'm using this smaller group of people to get a rough guess (an **estimate**) of what the conversion rate will be in the whole population. I also know that there's going to be some variation from sample to sample, even if they ultimately have the same **true** conversion rate. If a take a fair coin (50/50 chance of getting heads or tails) and flip it 100 times, I won't always get 50 heads and 50 tails exactly. If I do this twice and I get 51 heads and 49 tails in one go (51% success) and 48 heads and 52 tails in one go (48% success), I won't throw up my hands and say the coin I flipped in the first trial was different from the coin I flipped in the second one. In a similar way, it would be possible for me to get two different conversion rates for my registration prompts with and without text without their **true** conversion rates actually being different. At the end of the day, when we act on the conclusions we draw from an A|B test or from any other statistical test, we're still just making our best guess, given the evidence. What understanding the math behind A|B testing gets me is some sense of whether a difference in conversion rate would constitute enough evidence for me to "bet" on my variation and release it to all of my customers.

2. Why do we use a "binomial distribution"?
We define conversion rate as the number of "conversions" or "successes" over the number of "views" or "trials". The term "binomial distribution" makes the concept sound more complicated than it is. Simply put, the binomial distribution tells you what the probability of obtaining n successes is in N independent trials, where you each trial can only be a success or a failure. Basically, if you can frame something in terms of a success or failure (with some probability of succeeding), then you can model it as a binomial distrubtion. In the case of conversion rates, a user converting is a success and a user not converting is a failure. We consider a "trial" to be user coming to our site, a "success" to be them buying something and a "failure" to be leaving the site without buying anything. We use the binomial distribution to understand how likely it is to get a given overall conversion rate across a lot of users, given what we expect the conversion rate for one user to be.

3. Where do all the parts in the Z score formula come from?
For this Z score formulation, we are relying on the Wald method for calculating the standard error of a binomial distribution. Why we would or wouldn't use the Wald method is beyond the scope of this post (look out for a future post about this), but one of the main things to know about this method is it assumes that the binomial distribution can be approximated with a normal distribution. 

Back to the Z-score formula. If you recall from any early statistics class,  the Z-score of an observation is the equal to the observation - the mean of all observations divided by the standard deviation [INSERT FORMULA FORMATTED Z = (X - E[X])/sigma(X)]. As mentioned in section (1), the quantity of interest for us is the difference between the conversion rate of the control (A) and the variation (B). So for every quantity in the Z score formula, we'll be looking at the quantity for the difference in the conversion rates between variation A and variation B. The first term, X is simple:

X = the difference between our observed values p_a and p_b (p_a - p_b) <=> p_a - p_b

To find what value we should use for E[X], we have to look at our null and alternate hypotheses. We haven't previously talked about these explicitly, but we can formulate them now. The quantity of interest for us is the difference between p_a and p_b. According to [Wolfram](http://mathworld.wolfram.com/NullHypothesis.html), the null hypothesis is "usually that observations are a result of chance". In our case, if the difference that we observe between p_a and p_b were the result of chance, then we would essentially be saying that we believe that p_a and p_b are actually equal, even if in the specific samples we saw they were slightly different. So we get:

H_0: p_a = p_b <=> p_a - p_b = 0
When we do a test for significance, we **assume** the null hypothesis is true and then figure out how likely (or unlikely) our observations would be under that assumption. So in calculating our Z score, we assume that E[X] = E[p_a-p_b] = 0.

Now for our final term. The denominator for the Z_score calculation should be our [sample standard deviation or standard error of teh mean ????]. We know that for variance terms Var(A + B) = Var(A-B) =  Var(A) + Var(B). We also know that we're using Standard Error instead of standard deviation, so we get STDERR(p_a-p_b) = SQRT(STDERR(p_a)^2 + STDERR(p_b)^2).

Using the Wald method specified in the Amazon article, we get:

Putting all these terms together, we get the final Z score formula provided by Amazon.