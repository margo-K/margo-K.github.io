A short introduction to one way to analyze A|B tests

In trying to find resources for my colleagues about A|B testing, I came across a really wide variety of approaches for analyzing/setting up A|B tests. In this post, I'll try to explain one of the simplest ones - the Z test with a wald interval. This is the approach used in this post from Amazon about their A|B testing tool [The Math Behind A|B Testing](https://developer.amazon.com/public/apis/manage/ab-testing/doc/math-behind-ab-testing). I should also start by saying that if the content of this post seems a bit basic to you, you're probably not the target audience. Data scientists and statisticians - this post is not for you - it is for your colleagues, many of whom have had some statistics and would benefit from knowing how A|B testing relates to what they learned in school. This post is also not meant to be a full explanation of the intuition behind A|B testing but instead is meant to provide a supplement to more intensive study. I intend to provide a few signposts and key words so that any reader can better navigate all the other great resources out there.

##Let's start with an example.
Say you run a news website which can be browsed without registration, but registering allows readers to get content recommendations and premium content and it also allows you to gather more information that you can use to tailor content to users. Thus, you want to encourage more users to register, so you decide to try adding a short block of text under the registration prompt explaining to users what advantages they will get if they register. Let's call the normal registration prompt Version A and the prompt with the additional information Version B.

##What are we trying to find out with our A|B test.
 Normally, when you run an A|B test, you are trying to find out which of two alternatives (A & B) results in a better conversion rate. Often, folks will just compare the two conversion rates themselves (aggregate over all time) and say 
"Gee! The conversion rate was 15% before and now it's 10% - this change obviously isn't working". The problem with this is it doesn't take into account a few important factors, not the least of which is the number of samples (or "views") we're looking at in each case. If we just started an A|B test and only 10 people have seen the change we're trying to test, our 10% conversion rate (i.e. 1 person) doesn't mean a whole lot. It's easy to see that a conversion rate of 10% in a group of 100,000 people carries more weight than the same conversion rate in a group of 10 people, but when the numbers are a bit less extreme it can sometimes be a bit tougher. Paired with that is the fact that in general we want to be able to say something about the **true** conversion rate - when I'm running an A|B test, I don't just want to know whether version B performs better than version A for the specific people who saw it during our test - I want to know whether it will perform better for all of my customers. In statistical terms, I want to know about the **population** not just in a smaller **sample**. I'm using this smaller group of people to get a rough guess (an **estimate**) of what the conversion rate will be in the larger group. I know that there's some natural variation that's going to happen (when I flip a coin 100 times, I won't always get 50 heads and 50 tails exactly, even if I have a 50/50 chance of getting either. If I get 48 heads and 52 tails, I'm probably still fine calling this a "fair" coin), so what the hypothesis test framework gets us is a way to be more rigorous about this - when should I be reasonably certain that some difference in the conversion rate between the people who see my standard site and the people who see my variation constitutes enough evidence for me to "bet" on a variation and release it to the whole site? 

2. Why do we use a "binomial distribution"?
We define conversion rate as the number of "conversions" or "successes" over the number of "views" or "trials". The term "binomial distribution" makes the concept sound more complicated than it is. Simply put, the binomial distribution tells you what the probability of obtaining n successes is in N independent trials, where you each trial can only be a success or a failure. Basically, if you can frame something in terms of a success or failure (with some probability of succeeding), then you can model it as a binomial distrubtion. In the case of conversion rates, a user converting is a success and a user not converting is a failure. We consider a "trial" to be user coming to our site, a "success" to be them buying something and a "failure" to be leaving the site without buying anything. We use the binomial distribution to understand how likely it is to get a given overall conversion rate across a lot of users, given what we expect the conversion rate for one user to be.

3. Where do all the parts in the Z score formula come from?
For this Z score formulation, we are relying on the Wald method for calculating the standard error of a binomial distribution. Why we would or wouldn't use the Wald method is beyond the scope of this post (look out for a future post about this), but one of the main things to know about this method is it assumes that the binomial distribution can be approximated with a normal distribution. 

Back to the Z-score formula. If you recall from any early statistics class,  the Z-score of an observation is the equal to the observation - the mean of all observations divided by the standard deviation [INSERT FORMULA FORMATTED Z = (X - E[X])/sigma(X)]. As mentioned in section (1), the quantity of interest for us is the difference between the conversion rate of the control (A) and the variation (B). So for every quantity in the Z score formula, we'll be looking at the quantity for the difference in the conversion rates between variation A and variation B. The first term, X is simple:

X = the difference between our observed values p_a and p_b (p_a - p_b) <=> p_a - p_b

To find what value we should use for E[X], we have to look at our null and alternate hypotheses. We haven't previously talked about these explicitly, but we can formulate them now. The quantity of interest for us is the difference between p_a and p_b. According to [Wolfram](http://mathworld.wolfram.com/NullHypothesis.html), the null hypothesis is "usually that observations are a result of chance". In our case, if the difference that we observe between p_a and p_b were the result of chance, then we would essentially be saying that we believe that p_a and p_b are actually equal, even if in the specific samples we saw they were slightly different. So we get:

H_0: p_a = p_b <=> p_a - p_b = 0
When we do a test for significance, we **assume** the null hypothesis is true and then figure out how likely (or unlikely) our observations would be under that assumption. So in calculating our Z score, we assume that E[X] = E[p_a-p_b] = 0.

Now for our final term. The denominator for the Z_score calculation should be our [sample standard deviation or standard error of teh mean ????]. We know that for variance terms Var(A + B) = Var(A-B) =  Var(A) + Var(B). We also know that we're using Standard Error instead of standard deviation, so we get STDERR(p_a-p_b) = SQRT(STDERR(p_a)^2 + STDERR(p_b)^2).

Using the Wald method specified in the Amazon article, we get:

Putting all these terms together, we get the final Z score formula provided by Amazon.